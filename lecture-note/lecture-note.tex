\documentclass[11pt,a4paper,oneside,microtype,chapter,nokorean]{oblivoir}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\kwtrue}{\textsl{true}}
\newcommand{\kwfalse}{\textsl{false}}

% New definitions
\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicassert{\texttt{assert}}
\algnewcommand\Assert[1]{\State \algorithmicassert(#1)}%
% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}%
\algtext*{EndSwitch}%
\algtext*{EndCase}%

\begin{document}

\title{CS500 Lecture Note}
\author{Jeehoon Kang}
\maketitle


\chapter{Recurrences}

\section{Introduction}

\paragraph{Study guide}

I recommend you to study recurrences like this:

\begin{itemize}
\item Read the textbook's Section 11 (pages 87-98).
\item Read \Cref{sec:examples} to see how proofs work.
\item Solve the examples and exercises in the textbook's Sections 11.4, 11.5 (pages 91-98) on
  yourselves.
\end{itemize}


\paragraph{Exam guide}

Here are a few tips for the quizzes and exams.

\begin{itemize}
\item The most foundational way to solve a recurrence is by induction (what is called ``substitution
  method'' in the textbook).  Even if someone is very pedantic, s/he should accept proofs by
  induction as far as s/he believes in Mathematics.

\item ``Tree method'' and ``brick method'' are lemmas that deal with all the difficulties of proof
  by induction.  In the exams, you can freely use these methods as lemmas, except when the problem
  specifically asks you to prove it by induction.

\item Tree method: analyzing the recursion tree, and then deriving a closed-form solution.

\item Brick method says:
  \begin{itemize}
  \item If a recursion tree is root-dominated, then the total work is asymptotically equivalent to
    the work at the root.
  \item If a recursion tree is leaf-dominated, then the total work is asymptotically equivalent to
    the number of leaves.
  \end{itemize}

\item You can assume the grader already knows what ``recursion tree'' and ``level'' means, and
  assume the root is at the level $0$ (unless specified otherwise).  But you should not assume the
  grader is aware of \emph{e.g.} what $C(v)$ or $D(v)$ means for recursion trees.
\end{itemize}


\section{Examples}
\label{sec:examples}


\begin{example}[Balanced] Suppose $W(n) = 2W(n/2) + O(n)$.  Prove $W(n) = O(n~lg~n)$.
\end{example}

\begin{proof}
  We prove using the tree method, \emph{i.e.} analyzing the recursion tree and deriving a
  closed-form solution (Example 11.4).

  \begin{itemize}
  \item By definition, there exists $c_1,c_2$ such that $W(n) \le 2W(n/2) + c_1 n + c_2.$
  \item (Draw the recursion tree.)
  \item We can deduce from the tree that for the level $i$, there are $2^i$ nodes and the cost of
    each node is $c_1 (n (1/2)^i) + c_2$.  (To be pedantic, we can prove it by induction.)
  \item Thus the cost of the level $i$ is $2^i \times (c_1 (n (1/2)^i) + c_2) = c_1 n + 2^i c_2$.

  \item Since the recursion tree has $lg~n + 1$ levels, we have:
    \begin{align*}
      W(n)
      = &~ \sum_{i=0}^{lg~n} (c_1 n + 2^i c_2) & \mbox{($i=0$ for the root, $i=lg~n$ for the
                                                 leaves)} \\
      \le &~ c_1 n (lg~n + 1) + 2^{lg~n + 1} c_2 \\
      = &~ c_1 n (lg~n + 1) + 2 n c_2 \\
      = &~ O(n~lg~n).
    \end{align*}
  \end{itemize}
\end{proof}


\begin{example}[Root-dominated] Suppose $W(n) = 2W(n/2) + O(n^2)$.  Prove $W(n) = O(n^2)$.
\end{example}

\begin{proof}
  Prove by exploiting the fact that $W(n)$ is root-dominated (Example 11.5).

  \begin{itemize}
  \item By definition, there exists $c_1,c_2$ such that $W(n) \le 2W(n/2) + c_1 n^2 + c_2.$
  \item Let $C(v)$ be the cost of node $v$ in the \emph{recursion tree}.  If $v$ is a node of size
    $n$, then $C(v) = c_1 n^2 + c_2$, and
    $\sum_{u \in D(v)} C(u) = 2(c_1 (n/2)^2 + c_2) = 2 c_1 (n/2)^2 + 2 c_2$, where by $D(v)$ we mean
    the set of children of $v$ in the recursion tree.

    (Caution: we assume the grader already knows what ``recursion tree'' means, but you should
    define $C(v)$ and $D(v)$ on yourselves.)
  \item We have:
    \begin{align*}
      \dfrac{\sum_{u \in D(v)} C(u)}{C(v)}
      = &~ \dfrac{2 c_1 (n/2)^2 + 2 c_2}{c_1 n^2 + c_2} \\
      = &~ \dfrac{1 + (4 c_2 / c_1) / n^2}{2 + (2 c_2 / c_1) / n^2} & \mbox{(by dividing by $c_1 n^2 / 2$)} \\
      \le &~ 3/4. & \mbox{(if $(4 c_2 / c_1) / n^2 \le 1/2$)}
    \end{align*}
  \item So if $(4 c_2 / c_1) / n^2 \le 1/2$, or $\sqrt{8 c_2 / c_1} \le n$, then when walking down
    the tree, the cost of a level in the recursion tree decreases more than a factor of $4/3$.
  \item Thus the root dominates the cost, and $W(n) = O(C(n)) = O(c_1 n^2 + c_2) = O(n^2)$.
  \end{itemize}
\end{proof}


\begin{example}[Leaf-dominated] Suppose $W(n) = 2W(n/2) + O(\sqrt{n})$.  Prove $W(n) = O(n)$.
\end{example}

\begin{proof}
  Prove by exploiting the fact that $W(n)$ is leaf-dominated (Example 11.6).

  \begin{itemize}
  \item By definition, there exists $c_1,c_2$ such that $W(n) \le 2W(n/2) + c_1 \sqrt{n} + c_2.$
  \item Let $C(v)$ be the cost of node $v$ in the \emph{recursion tree}.  If $v$ is a node of size
    $n$, then $C(v) = c_1 \sqrt{n} + c_2$, and
    $\sum_{u \in D(v)} C(u) = 2(c_1 \sqrt{n/2} + c_2) = \sqrt{2} c_1 \sqrt{n} + 2 c_2$, where by
    $D(v)$ we mean the set of children of $v$ in the recursion tree.
  \item We have:
    \begin{align*}
      \dfrac{\sum_{u \in D(v)} C(u)}{C(v)}
      = &~ \dfrac{\sqrt{2} c_1 \sqrt{n} + 2 c_2}{c_1 \sqrt{n} + c_2} \\
      = &~ \dfrac{\sqrt{2} + (2 c_2 / c_1) / \sqrt{n}}{1 + (c_2 / c_1) / \sqrt{n}} & \mbox{(by dividing by $c_1 \sqrt{n}$)} \\
      \ge &~ \sqrt{2}.
    \end{align*}
  \item So when walking down the tree, the cost of a level in the recursion tree increases more than
    a factor of $\sqrt{2}$.
  \item Thus the leaves dominate the cost, and $W(n) = O(\mbox{the number of leaves}) = O(n)$.
  \end{itemize}
\end{proof}


\begin{example}[Induction] Suppose $W(n) = W(n/2) + O(n)$.  Prove $W(n) = O(n)$.
\end{example}

\begin{proof}
  First of all, by definition, there exists $c_1,c_2$ such that $W(n) \le W(n/2) + c_1 n + c_2$ for
  $n > 1$, and $W(1) = c_2$.  We prove $W(n) \le (2 c_1 + c_2) n + c_2$ by induction.

  \begin{itemize}
  \item Base case $n=1$: We have $W(n) = W(1) \le c_2 \le (2 c_1 + c_2) n + c_2$.

  \item Inductive case for $n$: We have:

    \begin{align*}
      W(n)
      \le &~ W(n/2) + c_1 n + c_2 & \mbox{(by assumption)} \\
      \le &~ ((2 c_1 + c_2) (n/2) + c_2) + c_1 n + c_2 & \mbox{(by inductive hypothesis)} \\
      = &~ c_1 n + (c_2 / 2) n + c_1 n + 2 c_2 \\
      = &~ (2 c_1 + c_2)n + c_2 (2 - n/2) \\
      \le &~ (2 c_1 + c_2)n + c_2. & \mbox{(since $n \ge 2$)}
    \end{align*}

  \item By induction, we have $W(n) \le (2 c_1 + c_2) n + c_2$ for all $n$.  Thus $W(n) = O(n)$.
  \end{itemize}
\end{proof}

\begin{example}[Root-dominated, advanced] Suppose $W(n) = W(\sqrt{n}) + W(n/2) + n$.  Prove
  $W(n) = O(n)$.
\end{example}


\begin{proof}
  Prove by exploiting the fact that $W(n)$ is root-dominated (page 95).

  \begin{itemize}
  \item Let $C(v)$ be the cost of node $v$ in the \emph{recursion tree}.  If $v$ is a node of size
    $n$, then $C(v) = n$, and $\sum_{u \in D(v)} C(u) = \sqrt{n} + (n/2)$, where by $D(v)$ we mean
    the set of children of $v$ in the recursion tree.
  \item We have:
    \begin{align*}
      \dfrac{\sum_{u \in D(v)} C(u)}{C(v)}
      = &~ \dfrac{\sqrt{n} + (n/2)}{n} \\
      = &~ \dfrac{1 + 2/\sqrt{n}}{2} & \mbox{(by dividing by $n/2$)} \\
      \le &~ 3/4. & \mbox{(if $2/\sqrt{n} \le 1/2$)}
    \end{align*}
  \item So if $2/\sqrt{n} \le 1/2$, or $16 \le n$, then when walking down the tree, the cost of a
    level in the recursion tree decreases more than a factor of $4/3$.
  \item Thus the root dominates the cost, and $W(n) = O(n)$.
  \end{itemize}
\end{proof}


\begin{example}[Leaf-dominated, advanced] Suppose $W(n) = W(n/2) + W(n/3) + 1$.  Prove
  $W(n) = O(n^{0.788})$.
\end{example}

\begin{proof}
  Prove by exploiting the fact that $W(n)$ is leaf-dominated (page 96).

  \begin{itemize}
  \item The recursion three is clearly leaf-dominated (exercise for the readers!).  Thus
    $W(n) = O(\mbox{the number of leaves})$.  Let's count it.

  \item Let $L(n)$ be the number of leaves when the size of the root is $n$.  Then we have
    $L(n) = L(n/2) + L(n/3)$.  Now it's clearly $L(n) = O(n)$, but we want to find a tighter bound.

  \item (Scribble: Let's guess that $L(n) = n^\beta$ for some $\beta$.  Then
    $n^\beta = (n/2)^\beta + (n/3)^\beta = n^\beta ((1/2)^\beta + (1/3)^\beta)$, and thus
    $1 = (1/2)^\beta + (1/3)^\beta$.  For this equation, there exists a real solution
    $\beta \le 0.788$.)

  \item As if we didn't scribble anything, just use the magic number $0.788$ and prove
    $W(n) = O(n^{0.788})$ by induction.
  \end{itemize}
\end{proof}


\chapter{Search Algorithms}

\section{Breadth-First Search (BFS)}

\paragraph{Algorithm}

Here is the BFS algorithm that finds all reachable vertices from a given vertex:

\begin{algorithm}
  \caption{Breadth-First Search Algorithm}\label{bfs}
  \begin{algorithmic}[1]
    \Procedure{BFS}{$G,v_i$} \Comment{BFS search algorithm}
    \State $\Call{BFSInner}{G,\emptyset,\{v_i\}}$
    \EndProcedure
    \Statex
    \Procedure{BFSInner}{$G=(V,E),X,F$} \Comment{A helper function for BFS}
    \If{$F = \emptyset$} \Comment{(needs: set empty predicate)}
    \State \textbf{return} $X$
    \EndIf
    \State $X' = X \cup F$ \Comment{(needs: set union)}
    \State $F' = N^+_G(F) \setminus X'$ \Comment{(needs: set flatten, set minus)}
    \State $\Call{BFSInner}{G,X',F'}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The above algorithm omits to implement the set operations highlighted above.  We present two
strategies for implementing sets: representing them as a boolean sequence, or as an element
sequence.  We begin with representing both $X$ and $F$ as boolean sequences.


\paragraph{Representing Set as a Boolean Sequence}

Let $V = \{v_0,v_1,\cdots,v_{|V|-1}\}$.  Then the sets $X, F \subseteq V$ can be represented as a
sequence of booleans of length $|V|$, where a sequence $S \in \mathbb{S}_{\mathbb{B}}$ represents
the set consisting of those elements $v_i$ for which $S_i$ is true.  For example, if
$V = \{v_0,v_1,v_2\}$, then $\langle \kwfalse, \kwtrue, \kwtrue \rangle$ represents the set
$\{v_1,v_2\}$.

Set operations are defined as follows:

\begin{itemize}
\item $\texttt{empty} = \texttt{tabulate}~(\lambda \_.~\kwfalse)~|V|$
\item $\texttt{is\_empty}(X) = \neg (\texttt{reduce}~\lor~\kwfalse~X)$
\item $\texttt{union}~A~B = \texttt{combine}~\lor~A~B$
\item $\textrm{minus}~A~B = \texttt{combine}~(\lambda a~b.~a \land \neg b)~A~B$
\item $\texttt{flatten}~\mathcal{A} = \texttt{reduce}~\texttt{union}~\texttt{empty}~\mathcal{A}$
\end{itemize}

By plugging in the above set operations, we have a complete BFS implementation.  It has $O(|V|^2)$
work and $O(|V|~lg~|V|)$ span.


\paragraph{Representing $F$ as a Sorted Index Sequence}

Representing $F$ as a boolean sequence is wasteful because, regardless of how many elements are in
$F$, it will iterate over all $|V|$ vertices at line 9, which dominates the work of the entire
algorithm.

To optimize line 9, we represent $F$ as a sorted sequence of indexes instead of as a sequence of
booleans.  The set $F \subseteq V$ can be represented as a sorted sequence of indexes whose
corresponding elements are in $F$.  For example, if $V = \{v_0,v_1,v_2\}$, then both
$\langle 1,2 \rangle$ and $\langle 2,1 \rangle$ represent the set $\{v_1,v_2\}$.

Set operations are defined as follows (here, $F,A,B$ are represented as a sorted index sequence and
$X$ as a boolean sequence):

\begin{itemize}
\item $\texttt{convert}~X = \texttt{map}~\texttt{fst}~(\texttt{filter}~\texttt{snd}~(\texttt{mapi}~(\lambda i,b. (i,b))~X))$
\item $\texttt{convert}~F = (\texttt{let}~F' = \texttt{sort}~F~\texttt{in}~\cdots)$
\item $\texttt{empty} = \langle~\rangle$
\item $\texttt{is\_empty}(F) = (|F| = 0)$
\item $\texttt{union}~A~B = \texttt{merge}~A~B$
\item $\textrm{minus}~F~X = \texttt{filter}~(\lambda f.~\neg X_f)~F$
\item $\texttt{flatten}~\mathcal{A} = \texttt{reduce}~\texttt{union}~\texttt{empty}~\mathcal{A}$
\end{itemize}

By plugging in the above set operations, we have another complete BFS implementation, whose work is
$O(|E|~lg~|V|)$ and span is $O(d~lg^2~|V|)$, where $d$ is the maximum length from $v_i$ in $G$.


\paragraph{Correctness} The following lemma on \textsc{BFSInner} is the key for BFS's correctness:

\begin{lemma}[Inductive invariant of \textsc{BFSInner}] In the recursion tree of
  $\textsc{BFS}(G,v_i)$, for each invocation $\textsc{BFSInner}(G,X,F)$, the following condition
  holds:
  \[ \forall v.~\mbox{($v$ is reachable from $v_i$ in $G$)} \iff (v \in X~\lor~\mbox{($v$ is
      reachable from $F$ in $G \setminus X$)}).
  \]  
\end{lemma}
\begin{proof}
  By induction.  For the base case, the first invocation $\textsc{BFSInner}(G,\emptyset,\{v_i\})$
  trivially satisfies the above condition.  For the inductive case, consider the invocation
  $\textsc{BFSInner}(G,X',F')$ from another one $\textsc{BFSInner}(G,X,F)$.

  The remainder of the proof is left as homework.  (Question: how did we find the inductive
  invariant?)
\end{proof}

From the above lemma follows the correctness of the algorithm:

\begin{theorem}[Correctness of \textsc{BFS}] For any $G$, $v_i$, and $v$, $v$ is reachable from
  $v_i$ in $G$ if and only if $v \in \textsc{BFS}(G,v_i)$.
\end{theorem}



\section{Depth-First Search (DFS)}

\paragraph{Algorithm}

Here is the DFS algorithm that finds all reachable vertices from a given vertex:

\begin{algorithm}
  \caption{Depth-First Search Algorithm}\label{dfs}
  \begin{algorithmic}[1]
    \Procedure{DFS}{$G,v_i$} \Comment{DFS search algorithm}
    \State $\Call{DFSInner}{G,\emptyset,\langle v_i\rangle}$
    \EndProcedure
    \Statex
    \Procedure{DFSInner}{$G=(V,E),X,S$} \Comment{A helper function for DFS}
    \Switch {$\textrm{splitHead}(S)$}
    \Case {\textrm{None}}
    \State $X$
    \EndCase
    \Case {$\textrm{Some}(v,S_t)$}
    \State $X' = X \cup \{v\}$
    \State $S' = (v \in X~\texttt{?}~\langle \rangle~\texttt{:}~N^+_G(v))~\texttt{++}~S_t$
    \State $\Call{DFSInner}{G,X',S'}$
    \EndCase
    \EndSwitch
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The sequence $S$ (representing a stack) is implemented with a linked list.


\paragraph{Cost}

In the recursion tree of \textsc{DFS}, there are at most $|E|$ invocations of \textsc{DFSInner}, and
the vertices in total are inserted into the stack ($S$) at most $|E|$ times.  Thus \textsc{DFS} has
$O(|E|)$ work.  Since the algorithm is completely sequential, it has $O(|E|)$ span.



\paragraph{Correctness} The correctness proof of DFS is similar to that of BFS.



\section{Dijkstra's Algorithm}

\paragraph{Algorithm}

Here is the Dijkstra's algorithm that finds the shortest path weight from a given vertex to all
vertices, assuming that all edge weights are non-negative.


\begin{algorithm}
  \caption{Dijkstra's Algorithm}\label{dijkstra}
  \begin{algorithmic}[1]
    \Procedure{Dijkstra}{$G,v_i$} \Comment{Dijkstra's shortest path algorithm}
    \State $D_0 = \langle \infty, \cdots, \infty, 0, \infty, \cdots, \infty \rangle$ \Comment{$\infty$ except for the initial vertex}
    \State $\Call{DijkstraInner}{G,D_0,\emptyset,\{v_i\}}$
    \EndProcedure
    \Statex
    \Procedure{DijkstraInner}{$G=(V,E),D,X,F$} \Comment{A helper function for Dijkstra}
    \If{$F = \emptyset$}
    \State \textbf{return} $D$
    \EndIf
    \State Let $f_0 \in F$ be a frontier with the minimum distance $D_{f_0}$
    \State $D' = \texttt{combine}~\textrm{min}~D~(N^+_G(f_0) + D_{f_0})$ \Comment{(abusing a lot of notations)}
    \State $X' = X \cup \{f_0\}$
    \State $F' = (F \cup N^+_G(f_0)) \setminus X'$
    \State $\Call{DijkstraInner}{G,D',X',F'}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\paragraph{Cost}

When representing $X$ as a boolean sequence and $F$ as a sorted index sequence, Dijkstra's
algorithm's work is $O(|V|^2)$ and span is $O(|V|~lg~|V|)$.  By using a more sophisticated data
structure (e.g. the priority queue), we can optimize (work-wise) Dijkstra's algorithm.



\paragraph{Correctness} The following lemma on \textsc{DijkstraInner} is the key for Dijkstra's
correctness:

\begin{lemma}[Inductive invariant of \textsc{DijkstraInner}] Suppose $\delta_G(v,w)$ is the shortest
  path weight from $v$ to $w$ in $G$.  In the recursion tree of $\textsc{Dijkstra}(G,v_i)$, for each
  invocation $\textsc{DijkstraInner}(G,D,X,F)$, the following conditions holds:
  \begin{itemize}
  \item $X \cap F = \emptyset$;
  \item $\forall v \notin X \cup F, D_v = \infty$;
  \item $\forall v\in X, w \notin X.~D_v \le D_w$;
  \item $\forall v \in X.~\delta_G(v_i,v) = D_v$; and
  \item
    $\forall v \notin X.~\delta_G(v_i,v) = \textrm{min}_{f \in F} (D_f + (\delta_{G \setminus
      X}(f,v)))$.
  \end{itemize}

  (Question: how did we find the inductive invariant?)
\end{lemma}
\begin{proof}
  By induction.

  \begin{itemize}
  \item In the initial case, we have $(D, X, F) = (D_0, \emptyset, \{v_i\})$.  All conditions trivially hold.
  \item In the inductive case, we have:
    \begin{align*}
      \forall v.~D'_v =&~ \textrm{min}(D_v, D_{f_0} + w_G(f_0,v)) \\
      X' =&~ X \cup \{f_0\} \\
      F' =&~ (F \cup N^+_G(f_0)) \setminus X'~,
    \end{align*}
    where $D', X', F'$ are the arguments for the recursive invocation of \textsc{DijkstraInner}.
    Now we prove that the conditions hold for $D', X', F'$, assuming they hold for $D, X, F$ as the
    inductive hypothesis.

    \begin{itemize}
    \item $X' \cap F' = X' \cap ((F \cup N^+_G(f_0)) \setminus X') = \emptyset$.

    \item If $v \notin X' \cup F'$, we have $v \notin X \cup F \cup N^+_G(f_0)$ and thus
      $v \notin X \cup F$ and $v \notin N^+_G(f_0)$.  Then we have:
      %
      \begin{align*}
        D'_v
        =&~ \textrm{min}(D_v, w_G(f_0,v)) \\
        =&~ \textrm{min}(\infty, w_G(f_0,v)) & \mbox{(by $v \notin X \cup F$ and IH)} \\
        =&~ \textrm{min}(\infty, \infty) & \mbox{(by $v \notin N^+_G(f_0)$)} \\
        =&~ \infty.
      \end{align*}

    \item If $v \in X'$, then $D'_v \le D_{f_0}$.  If $w \notin X'$, then $D'_w \ge D_{f_0}$.  Thus
      $\forall v\in X, w \notin X.~D_v \le D_w$.

    \item If $v \in X'$, then either $v \in X$ or $v = f_0$.

      If $v \in X$, then $\delta_G(v_i,v) = D_v$ from the IH's fourth condition, and $D_v = D'_v$
      from the IH's third condition.

      If $v = f_0$, we have:
      \begin{align*}
        \delta_G(v_i,v) =&~ \textrm{min}_{f \in F} (D_f + (\delta_{G \setminus X}(f,v))) & \mbox{(by the IH's
                                                                                           fifth condition)} \\
        =&~ D_{f_0} & \mbox{($\because [\forall f \in F.~D_{f_0} \le D_f]$ and
                      $\delta_{G \setminus X}(f_0,v) = 0$)} \\
        =&~ D'_{f_0}~. & \mbox{($\because 0 \le w_G(f_0,f_0)$)}
      \end{align*}

    \item If $v \notin X'$, then we have $v \notin X$ and $v \neq f_0$.  We have:
      \begin{align*}
        \delta_G(v_i,v) =&~ \textrm{min}_{f \in F} (D_f + (\delta_{G \setminus X}(f,v))) & \mbox{(by the IH's
                                                                                           fifth cond.)} \\
        =&~ \textrm{min} [ D_{f_0} + (\delta_{G \setminus X}(f_0,v)), \\
                         &~ ~~~~~~\textrm{min}_{f \in F \setminus \{f_0\}} (D_f + \delta_{G \setminus X}(f,v)) ] \\
        =&~ \textrm{min} [ D_{f_0} + \textrm{min}_{u \in N^+_G(f_0) \setminus X'}(w_G(f_0,u) + \delta_{G \setminus X'}(u,v)), & \mbox{($\because f_0 \neq v$)} \\
                         &~ ~~~~~~\textrm{min}_{f \in F \setminus \{f_0\}} (D_f + \delta_{G \setminus X}(f,v)) ] \\
        =&~ \textrm{min} [ D_{f_0} + \textrm{min}_{u \in N^+_G(f_0) \setminus X'}(w_G(f_0,u) + \delta_{G \setminus X'}(u,v)), & \mbox{($\because$ visiting $f_0$ is bad} \\
                         &~ ~~~~~~\textrm{min}_{f \in F \setminus \{f_0\}} (D_f + \delta_{G \setminus X'}(f,v)) ] & \mbox{as $D_{f_0} \le D_f$)} \\
        =&~ \textrm{min}_{f \in (N^+_G(f_0) \setminus X') \cup (F \setminus \{f_0\})} \\
        &~ ~~~~~~[\textrm{min}(D_{f_0} + w_G(f_0,f), D_f) + \delta_{G \setminus X'}(f,v)] \\
        =&~ \textrm{min}_{f \in F'} D'_f + \delta_{G \setminus X'}(f,v)
      \end{align*}
    \end{itemize}
  \end{itemize}
\end{proof}


From the above lemma follows the correctness of the algorithm:

\begin{theorem}[Correctness of \textsc{Dijkstra}] For any $G$, $v_i$, and $v$,
  $\textsc{Dijkstra}(G,v_i)_v$ is the shortest path weight from $v_i$ to $v$.
\end{theorem}



\section{Bellman-Ford's Algorithm}

\paragraph{Algorithm}

Here is the Bellman-Ford's algorithm that finds the shortest path weight from a given vertex to all
vertices, assuming that there are possibly edges with negative weight but no cycles of negative
distance.

\begin{algorithm}
  \caption{Bellman-Ford's Algorithm}\label{dijkstra}
  \begin{algorithmic}[1]
    \Procedure{Bellman-Ford}{$G,v_i$} \Comment{Bellman-Ford's shortest path algorithm}
    \State $D_0 = \langle \infty, \cdots, \infty, 0, \infty, \cdots, \infty \rangle$ \Comment{$\infty$ except for the initial vertex}
    \State $\Call{Bellman-FordInner}{G,D_0,0}$
    \EndProcedure
    \Statex
    \Procedure{Bellman-FordInner}{$G=(V,E),D,k$} \Comment{A helper function}
    \If{$k = |V|$}
    \State \textbf{return} $D$
    \EndIf
    \State Let $D' = [w \mapsto \textrm{min}(D_w, \textrm{min}_{v_j \in N^-_G(v_w)} (D_j + w_G(j,w)))]$
    \State $\Call{Bellman-FordInner}{G,D',k+1}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\paragraph{Cost}

In the recursion tree, an invocation of \textsc{Bellman-FordInner} has $O(|E|)$ work and $O(lg~|V|)$
span.  Thus the total work and span of \textsc{Bellman-Ford} is $O(|V||E|)$ and $O(|V|~lg~|V|)$.



\paragraph{Correctness} The following lemma on \textsc{Bellman-FordInner} is the key for Bellman-Ford's
correctness:

\begin{lemma}[Inductive invariant of \textsc{Bellman-FordInner}] In the recursion tree of
  $\textsc{Bellman-Ford}(G)$, for each invocation $\textsc{Bellman-FordInner}(G,D,k)$, the
  following condition holds:
  \[ \forall j.~D_j = \delta_G^k(v_i,v_j),
  \]  
  where $\delta_G^k(v_i,v_j)$ is the shortest distance of the paths from $v_i$ to $v_j$ in $G$ of
  length $\le k+1$.
\end{lemma}
\begin{proof}
  Homework.  (Question: how did we find the inductive invariant?)
\end{proof}

From the above lemma follows the correctness of the algorithm:

\begin{theorem}[Correctness of \textsc{Bellman-Ford}] For any $G$, $i$, and $j$,
  $\textsc{Bellman-Ford}(G,v_i)_{j}$ is the shortest path weight from $v_i$ to $v_j$.
\end{theorem}



\section{Floyd-Warshall's Algorithm}

\paragraph{Algorithm}

Here is the Floyd-Warshall's algorithm that finds the shortest path weight from all vertices to all
vertices, assuming that there are possibly edges with negative weight but no cycles of negative
distance.

\begin{algorithm}
  \caption{Floyd-Warshall's Algorithm}\label{dijkstra}
  \begin{algorithmic}[1]
    \Procedure{Floyd-Warshall}{$G = (V,E)$} \Comment{Floyd-Warshall's shortest path algorithm}
    \State $D_0 = E$ \Comment{The initial distance matrix}
    \State $\Call{Floyd-WarshallInner}{G,D_0,0}$
    \EndProcedure
    \Statex
    \Procedure{Floyd-WarshallInner}{$G=(V,E),D,k$} \Comment{A helper function}
    \If{$k = |V|$}
    \State \textbf{return} $D$
    \EndIf
    \State Let $D' = [(i,j) \mapsto \textrm{min}(D_{i,j}, D_{i,k} + D_{k,j})]$
    \State $\Call{Floyd-WarshallInner}{G,D',k+1}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\paragraph{Cost}

In the recursion tree, an invocation of \textsc{Floyd-WarshallInner} has $O(|V|^2)$ work and $O(1)$
span.  Thus the total work and span of \textsc{Floyd-Warshall} is $O(|V|^3)$ and $O(|V|)$.


\paragraph{Correctness} The following lemma on \textsc{Floyd-WarshallInner} is the key for
Floyd-Warshall's algorithm's correctness:

\begin{lemma}[Inductive invariant of \textsc{Floyd-WarshallInner}] In the recursion tree of
  $\textsc{Floyd-Warshall}(G)$, for each invocation $\textsc{Floyd-WarshallInner}(G,D,k)$, the
  following condition holds:
  \[ \forall i,j.~D_{i,j} = \delta_G^k(v_i,v_j),
  \]
  where $\delta_G^k(v_i,v_j)$ is the shortest distance of the paths from $v_i$ to $v_j$ in $G$ where
  the intermediate vertices are in $\{0,1,\cdots,(k-1)\}$.
\end{lemma}
\begin{proof}
  Homework.  (Question: how did we find the inductive invariant?)
\end{proof}

From the above lemma follows the correctness of the algorithm:

\begin{theorem}[Correctness of \textsc{Floyd-Warshall}] For any $G$, $i$, and $j$,
  $\textsc{Floyd-Warshall}(G)_{i,j}$ is the shortest path weight from $v_i$ to $v_j$.
\end{theorem}




\chapter{On Limited Resources}

Until the midterm exam, we had mostly focused on the pure algorithms.  From now on, we will also
focus on the limited resources such as processors, cache, memory, etc.


\section{Brent's Theorem}

Let's first consider processors.  Fortunately, parallel algorithms enjoy the following property:

\begin{theorem}[Brent]
  Let $W$ and $S$ be the work and span of an algorithm.  Given $P$ processors, it is possible to
  execute the algorithm with the time complexity $T = O \Big( \dfrac{W}{P} + S \Big)$.
\end{theorem}

Brent's algorithm basically says, given $P$ processors and an algorithm with work $W$ and span $S$,
there exists a \textbf{scheduler} that assigns the jobs to the $P$ processors with time complexity
$O((W/P) + S)$.  For the proof of Brent's theorem, we refer the reader to a Wikipedia page:
\url{https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms}.


\paragraph{Example: Matrix Multiplication}

Multiplying two matrices of size $n \times n$ costs $O(n^3)$ work and $O(lg~n)$ span.  Given varying
number of processors, you can execute the algorithm with the time complexity as follows:

\begin{center}
\begin{tabular}{ c | c }
 \# of processors & time complexity \\
  \hline
 $O(1)$ & $O(n^3)$ \\
 $O(n)$ & $O(n^2)$ \\
 $O(n^2)$ & $O(n)$ \\
 $O(n^3 / (lg~n))$ & $O(lg~n)$ \\
 $O(n^3)$ & $O(lg~n)$ \\
 $O(\infty)$ & $O(lg~n)$
\end{tabular}
\end{center}



\section{Example: Systolic Array}

\paragraph{Example}

These videos are excellent examples of systolic array:
\url{https://www.youtube.com/watch?v=nzVyQzv8FG8} \url{https://www.youtube.com/watch?v=eK5fjuEFJu0}
Watching these videos, you will quite get the idea of systolic array.

FYI, ``systolic'' means ``relating to the phase of the heartbeat when the heart muscle contracts and
pumps blood from the chambers into the arteries.''
(\url{https://en.oxforddictionaries.com/definition/systolic}).


\paragraph{Algorithm}

Here is the systolic array algorithm:

\begin{algorithm}
  \caption{Systolic Array}\label{systolic}
  \begin{algorithmic}[1]
    \Procedure{Systolic}{$W,X$} \Comment{Systolic array algorithm}
    \State $I_0 = [(i,j) \mapsto 0]$ \Comment{Systolic input from $X$}
    \State $A_0 = [(i,j) \mapsto 0]$ \Comment{Accumulator}
    \State $O_0 = [(i,j) \mapsto 0]$ \Comment{Output ($W \times X$)}
    \State $\Call{SystolicInner}{W,X,0,I_0,A_0,O_0}$
    \EndProcedure
    \Statex
    \Procedure{SystolicInner}{$W,X,k,I,A,O$} \Comment{A helper function}
    \If{$k = 3n-1$}
    \State \textbf{return} $O$
    \EndIf
    \State $I' = [(i,j) \mapsto (i=0)~?~X(k-j,j)~:~I(i-1,j)]$
    \State $A' = [(i,j) \mapsto ((j=0)~?~0~:~A(i,j-1)) + (W(i,j) \times I'(i,j)))]$
    \State $O' = O[\forall i.~(i,k-n-i) \mapsto A(i,n-1)]$
    \State $\Call{SystolicInner}{W,X,k+1,I',A',O'}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

An invocation to \textsc{SystolicInner} costs $O(n^2)$ work and $O(1)$ span.  Thus the total cost of
\textsc{Systolic} is $O(n^3)$ work and $O(n)$ span.


\paragraph{Discussion}

Systolic array is a way of realizing the matrix multiplication algorithm with $n^2$ processors and
$O(n)$ time complexity, by $(i)$ placing the $n^2$ processors in square ($n \times n$), and $(ii)$
assigning the computation of $I(i,j)$, $A(i,j)$, and $O(i,j)$ to the $(i,j)$-th processor.  In other
words, you may think of systolic array as the combination of $(i)$ the matrix multiplication
algorithm and $(ii)$ a scheduling strategy for $n^2$ processors.

Then what's the difference between the systolic array and the general matrix multiplication
algorithm with the Brent's scheduler?  Notice that systolic array exhibits excellent spatial
locality: the computation within a processor requires only those values in the same or adjacent
processor.  Systolic array is designed so as to exploit this spatial locality, which greatly
improves the throughput in the real-world, resource-limited computers.  In contrast, Brent's theorem
says nothing about locality.

Systolic array is widely deployed in high-performance computer architecture.  For example, Google's
TPU has a systolic array of ${256}^2$ processors, which is able to multiply two matrices of size
$256 \times 256$ within $256 \times 3$ cycles (see \url{https://arxiv.org/pdf/1704.04760.pdf} for
more details).


\section{Odd-Even Merge Sort}

TODO


\section{Bitonic Sort}

TODO


\section{Dynamic Scheduling}

TODO



\end{document}
